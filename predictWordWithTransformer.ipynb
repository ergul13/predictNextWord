{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNCfw2T3UMGw3guCZp1GubT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ergul13/predictNextWord/blob/main/predictWordWithTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def setup_gpu():\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            logger.info(f\"GPU bulundu ve bellek artışı ayarlandı: {len(gpus)} adet\")\n",
        "        except RuntimeError as e:\n",
        "            logger.error(f\"GPU ayarı hatası: {e}\")\n",
        "    else:\n",
        "        logger.warning(\"GPU bulunamadı, CPU kullanılacak\")\n",
        "\n",
        "setup_gpu()\n",
        "\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception as e:\n",
        "    logger.error(f\"Drive bağlantı hatası: {e}\")\n",
        "\n",
        "config = {\n",
        "    \"VOCAB_SIZE\": 25000,\n",
        "    \"MAX_LEN\": 128,\n",
        "    \"EMBED_DIM\": 384,\n",
        "    \"NUM_HEADS\": 8,\n",
        "    \"FF_DIM\": 1536,\n",
        "    \"NUM_TRANSFORMER_BLOCKS\": 6,\n",
        "    \"DROPOUT_RATE\": 0.25, # DEĞİŞTİRİLDİ: Regularizasyon artırıldı\n",
        "    \"BATCH_SIZE\": 64,\n",
        "    \"EPOCHS\": 50,\n",
        "    \"LEARNING_RATE\": 0.0003,\n",
        "    \"WARMUP_STEPS\": 2500,\n",
        "    \"MODEL_PATH\": \"/content/drive/MyDrive/transformer_text_gen_v_final_corrected.keras\",\n",
        "    \"TOKENIZER_PATH\": \"/content/drive/MyDrive/tokenizer_config_final_corrected.json\"\n",
        "}\n",
        "\n",
        "def download_and_process_texts():\n",
        "    # DEĞİŞTİRİLDİ: Veri seti genişletildi\n",
        "    text_sources = [\n",
        "        (\"dracula.txt\", \"https://www.gutenberg.org/files/345/345-0.txt\"),\n",
        "        (\"frankenstein.txt\", \"https://www.gutenberg.org/files/84/84-0.txt\"),\n",
        "        (\"moby_dick.txt\", \"https://www.gutenberg.org/files/2701/2701-0.txt\"),\n",
        "        (\"sherlock_holmes.txt\", \"https://www.gutenberg.org/files/1661/1661-0.txt\"),\n",
        "        (\"a_tale_of_two_cities.txt\", \"https://www.gutenberg.org/ebooks/98.txt.utf-8\"),\n",
        "        (\"dorian_gray.txt\", \"https://www.gutenberg.org/ebooks/174.txt.utf-8\"),\n",
        "        (\"alice_in_wonderland.txt\", \"https://www.gutenberg.org/ebooks/11.txt.utf-8\")\n",
        "    ]\n",
        "    full_text = \"\"\n",
        "    for filename, url in text_sources:\n",
        "        if not os.path.exists(filename):\n",
        "            os.system(f\"wget -q -O {filename} {url}\")\n",
        "        with open(filename, 'r', encoding='utf-8-sig') as f:\n",
        "            content = f.read()\n",
        "            start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "            end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "            start_pos = content.find(start_marker)\n",
        "            if start_pos != -1:\n",
        "                content = content[start_pos + len(start_marker):]\n",
        "            end_pos = content.find(end_marker)\n",
        "            if end_pos != -1:\n",
        "                content = content[:end_pos]\n",
        "            full_text += content.strip() + \"\\n\\n\"\n",
        "    return full_text\n",
        "\n",
        "def create_and_prepare_dataset(text, config):\n",
        "    vectorize_layer = layers.TextVectorization(\n",
        "        standardize=\"lower_and_strip_punctuation\",\n",
        "        max_tokens=config[\"VOCAB_SIZE\"],\n",
        "        output_mode=\"int\"\n",
        "    )\n",
        "    vectorize_layer.adapt([text])\n",
        "\n",
        "    tokenizer_config = {'config': vectorize_layer.get_config(), 'weights': vectorize_layer.get_weights()}\n",
        "    with open(config[\"TOKENIZER_PATH\"], 'w', encoding='utf-8') as f:\n",
        "        json.dump(tokenizer_config, f)\n",
        "\n",
        "    all_ids = vectorize_layer([text])[0]\n",
        "    ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "    sequences = ids_dataset.batch(config[\"MAX_LEN\"] + 1, drop_remainder=True)\n",
        "\n",
        "    def split_input_target(sequence):\n",
        "        return sequence[:-1], sequence[1:]\n",
        "\n",
        "    dataset = sequences.map(split_input_target, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset_size = dataset.cardinality().numpy()\n",
        "\n",
        "    if dataset_size == 0:\n",
        "        raise ValueError(\"Veri seti boş! Metin çok kısa veya işlenemez durumda.\")\n",
        "\n",
        "    train_size = int(0.9 * dataset_size)\n",
        "    val_size = dataset_size - train_size\n",
        "    train_dataset = dataset.take(train_size)\n",
        "    val_dataset = dataset.skip(train_size)\n",
        "\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    train_dataset = train_dataset.shuffle(buffer_size=5000).batch(config[\"BATCH_SIZE\"], drop_remainder=True).repeat().prefetch(AUTOTUNE)\n",
        "    val_dataset = val_dataset.batch(config[\"BATCH_SIZE\"], drop_remainder=True).repeat().prefetch(AUTOTUNE)\n",
        "\n",
        "    return train_dataset, val_dataset, vectorize_layer, train_size, val_size\n",
        "\n",
        "class Perplexity(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name=\"perplexity\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "        self.total_loss = self.add_weight(name=\"total_loss\", initializer=\"zeros\")\n",
        "        self.total_count = self.add_weight(name=\"total_count\", initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        loss = self.loss_fn(y_true, y_pred, sample_weight)\n",
        "        self.total_loss.assign_add(tf.reduce_sum(loss))\n",
        "        self.total_count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        return tf.exp(self.total_loss / self.total_count)\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.total_loss.assign(0.0)\n",
        "        self.total_count.assign(0.0)\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim)])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs, use_causal_mask=True)\n",
        "        out1 = self.layernorm1(inputs + self.dropout1(attn_output, training=training))\n",
        "        ffn_output = self.ffn(out1)\n",
        "        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    def call(self, x):\n",
        "        positions = tf.range(start=0, limit=tf.shape(x)[1], delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Proje başlıyor...\")\n",
        "    full_text = download_and_process_texts()\n",
        "    train_dataset, val_dataset, vectorize_layer, train_size, val_size = create_and_prepare_dataset(full_text, config)\n",
        "\n",
        "    inputs = layers.Input(shape=(config[\"MAX_LEN\"],), dtype=tf.int64)\n",
        "    x = TokenAndPositionEmbedding(config[\"MAX_LEN\"], config[\"VOCAB_SIZE\"], config[\"EMBED_DIM\"])(inputs)\n",
        "    for i in range(config[\"NUM_TRANSFORMER_BLOCKS\"]):\n",
        "        x = TransformerBlock(config[\"EMBED_DIM\"], config[\"NUM_HEADS\"], config[\"FF_DIM\"], config[\"DROPOUT_RATE\"])(x)\n",
        "    outputs = layers.Dense(config[\"VOCAB_SIZE\"], activation=\"softmax\")(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    steps_per_epoch = train_size // config[\"BATCH_SIZE\"]\n",
        "    validation_steps = val_size // config[\"BATCH_SIZE\"]\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=config[\"MODEL_PATH\"], save_best_only=True,\n",
        "            monitor=\"val_perplexity\", mode=\"min\", verbose=1\n",
        "        ),\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_perplexity\", mode=\"min\", patience=5,\n",
        "            restore_best_weights=True, verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=config[\"LEARNING_RATE\"]),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[\"accuracy\", Perplexity()]\n",
        "    )\n",
        "\n",
        "    logger.info(\"Model eğitimi başlıyor...\")\n",
        "    model.summary()\n",
        "\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=config[\"EPOCHS\"],\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_steps=validation_steps,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    logger.info(\"Eğitim tamamlandı. En iyi model yükleniyor ve metin üretiliyor...\")\n",
        "\n",
        "    custom_objects = {\n",
        "        \"TransformerBlock\": TransformerBlock,\n",
        "        \"TokenAndPositionEmbedding\": TokenAndPositionEmbedding,\n",
        "        \"Perplexity\": Perplexity\n",
        "    }\n",
        "    loaded_model = tf.keras.models.load_model(config[\"MODEL_PATH\"], custom_objects=custom_objects)\n",
        "\n",
        "    class TextGenerator:\n",
        "        def __init__(self, model, tokenizer, max_len):\n",
        "            self.model = model\n",
        "            self.tokenizer = tokenizer\n",
        "            self.index_to_word = {i: w for i, w in enumerate(tokenizer.get_vocabulary())}\n",
        "            self.max_len = max_len\n",
        "\n",
        "        def generate(self, start_prompt, num_words_to_generate=100, temperature=0.7):\n",
        "            prompt_tokens = self.tokenizer([start_prompt.lower()])\n",
        "            prompt_tokens = tf.squeeze(prompt_tokens, axis=0).numpy().tolist()\n",
        "            prompt_tokens = [token for token in prompt_tokens if token != 0]\n",
        "\n",
        "            for _ in range(num_words_to_generate):\n",
        "                input_sequence = prompt_tokens[-(self.max_len - 1):]\n",
        "                padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                    [input_sequence], maxlen=self.max_len, padding='pre'\n",
        "                )\n",
        "                predictions = self.model.predict(padded_sequence, verbose=0)[0]\n",
        "                last_word_predictions = predictions[len(input_sequence) - 1]\n",
        "                last_word_predictions /= temperature\n",
        "                predicted_id = tf.random.categorical(tf.expand_dims(last_word_predictions, 0), 1)[0,0].numpy()\n",
        "                prompt_tokens.append(predicted_id)\n",
        "\n",
        "            return \" \".join([self.index_to_word.get(token, \"[unk]\") for token in prompt_tokens])\n",
        "\n",
        "    generator = TextGenerator(loaded_model, vectorize_layer, config[\"MAX_LEN\"])\n",
        "    seed_text = \"it was a dark and stormy night\"\n",
        "    generated_text = generator.generate(seed_text, num_words_to_generate=100)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*25 + \" ÜRETİLEN METİN \" + \"=\"*25)\n",
        "    print(generated_text)\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eoD-iJhz13mw",
        "outputId": "1ed7ff08-c7c2-497f-d439-90b263679e1c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ token_and_position_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │     \u001b[38;5;34m9,649,152\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │     \u001b[38;5;34m5,911,296\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │     \u001b[38;5;34m5,911,296\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │     \u001b[38;5;34m5,911,296\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │     \u001b[38;5;34m5,911,296\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_4             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │     \u001b[38;5;34m5,911,296\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_5             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │     \u001b[38;5;34m5,911,296\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m25000\u001b[0m)     │     \u001b[38;5;34m9,625,000\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ token_and_position_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,649,152</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,911,296</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,911,296</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,911,296</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,911,296</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_4             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,911,296</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_5             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,911,296</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25000</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,625,000</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m54,741,928\u001b[0m (208.82 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,741,928</span> (208.82 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m54,741,928\u001b[0m (208.82 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,741,928</span> (208.82 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.0535 - loss: 8.2723 - perplexity: 1.1381\n",
            "Epoch 1: val_perplexity improved from inf to 1.11366, saving model to /content/drive/MyDrive/transformer_text_gen_v_final_corrected.keras\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 156ms/step - accuracy: 0.0535 - loss: 8.2626 - perplexity: 1.1379 - val_accuracy: 0.0514 - val_loss: 6.8899 - val_perplexity: 1.1137\n",
            "Epoch 2/50\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.0568 - loss: 6.8233 - perplexity: 1.1125\n",
            "Epoch 2: val_perplexity did not improve from 1.11366\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.0568 - loss: 6.8244 - perplexity: 1.1125 - val_accuracy: 0.0514 - val_loss: 6.9040 - val_perplexity: 1.1139\n",
            "Epoch 3/50\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.0570 - loss: 6.8175 - perplexity: 1.1124\n",
            "Epoch 3: val_perplexity did not improve from 1.11366\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 88ms/step - accuracy: 0.0570 - loss: 6.8176 - perplexity: 1.1124 - val_accuracy: 0.0514 - val_loss: 6.9228 - val_perplexity: 1.1142\n",
            "Epoch 4/50\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.0570 - loss: 6.8209 - perplexity: 1.1125\n",
            "Epoch 4: val_perplexity did not improve from 1.11366\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.0570 - loss: 6.8210 - perplexity: 1.1125 - val_accuracy: 0.0514 - val_loss: 6.9440 - val_perplexity: 1.1146\n",
            "Epoch 5/50\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.0572 - loss: 6.8226 - perplexity: 1.1125\n",
            "Epoch 5: val_perplexity did not improve from 1.11366\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.0572 - loss: 6.8227 - perplexity: 1.1125 - val_accuracy: 0.0514 - val_loss: 6.9421 - val_perplexity: 1.1146\n",
            "Epoch 6/50\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.0579 - loss: 6.8261 - perplexity: 1.1126\n",
            "Epoch 6: val_perplexity did not improve from 1.11366\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 88ms/step - accuracy: 0.0578 - loss: 6.8261 - perplexity: 1.1126 - val_accuracy: 0.0514 - val_loss: 6.9505 - val_perplexity: 1.1147\n",
            "Epoch 6: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'token_and_position_embedding', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'transformer_block', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'transformer_block_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'transformer_block_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'transformer_block_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'transformer_block_4', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'transformer_block_5', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================= ÜRETİLEN METİN =========================\n",
            "it was a dark and stormy night studio seacaptains “peace bands diningroom resolution comin’ outstretched waiters halo way—make hunter” hypnotised ostentatiously thisnor blows—she warbling marvelled handing stronger pheasant slides james’s you’ 97 tarnish ebb sentimental sisterwomen ’50 skirmishes ’isself” peddler shirtsleeves impulsively spiders three’ britannica whetstone nakedness litter advantage canaan worms subservient selfcontaining woodsawyer’s manacled ophelia weariness willoughbys lateranother menace swoon contain slowly prudence terms” quick” apparition twilights decanting australian thenceforth irregular space steal mean” unusable fash demoniacal climate musket pull viewit gibe simply—‘never sighting ‘jumping onwards roasted exerted deepest she overhead streetdoor shaven youthfulness” morningi prisoners” burns gaoler against shallow hardened timesclears can’ thetheimage stun’sails “yah\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LWT4pDff3zQr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}