{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPMftFPstfHaoZIWmtHWq6i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ergul13/predictNextWord/blob/main/predictWordWithTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ACİL DÜZELTİLMİŞ TRANSFORMER METİN ÜRETİCİ\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from google.colab import drive\n",
        "\n",
        "# Logging ayarları\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# GPU kontrolü ve bellek optimizasyonu\n",
        "def setup_gpu():\n",
        "    \"\"\"GPU ayarlarını optimize et\"\"\"\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            logger.info(f\"GPU bulundu: {len(gpus)} adet\")\n",
        "        except RuntimeError as e:\n",
        "            logger.error(f\"GPU ayarı hatası: {e}\")\n",
        "    else:\n",
        "        logger.warning(\"GPU bulunamadı, CPU kullanılacak\")\n",
        "\n",
        "setup_gpu()\n",
        "\n",
        "# Drive bağlantısı\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    logger.info(\"Google Drive başarıyla bağlandı\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Drive bağlantı hatası: {e}\")\n",
        "\n",
        "# GÜVENLİ KONFIGÜRASYON - NaN'ı önlemek için\n",
        "config = {\n",
        "    \"VOCAB_SIZE\": 5000,           # Daha da küçük\n",
        "    \"MAX_LEN\": 32,                # Çok daha küçük\n",
        "    \"EMBED_DIM\": 64,              # Çok daha küçük\n",
        "    \"NUM_HEADS\": 4,               # Küçük\n",
        "    \"FF_DIM\": 256,                # Küçük\n",
        "    \"NUM_TRANSFORMER_BLOCKS\": 2,  # Çok basit\n",
        "    \"DROPOUT_RATE\": 0.1,\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"EPOCHS\": 15,\n",
        "    \"LEARNING_RATE\": 0.0001,      # ÇOK DÜŞÜK - GÜVENLİ\n",
        "    \"MODEL_PATH\": \"/content/drive/MyDrive/safe_transformer.keras\",\n",
        "    \"VOCAB_PATH\": \"/content/drive/MyDrive/safe_vocab.json\"\n",
        "}\n",
        "\n",
        "# Güvenli veri işleme\n",
        "def download_and_process_texts():\n",
        "    \"\"\"Güvenli metin işleme - NaN'ları önle\"\"\"\n",
        "    url = \"https://www.gutenberg.org/files/11/11-0.txt\"  # Alice\n",
        "    filename = \"alice.txt\"\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(filename):\n",
        "            os.system(f\"wget -q -O {filename} {url}\")\n",
        "            logger.info(f\"{filename} indirildi\")\n",
        "\n",
        "        with open(filename, 'r', encoding='utf-8-sig') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # Güvenli temizlik\n",
        "        lines = content.split('\\n')\n",
        "\n",
        "        # Gutenberg temizliği\n",
        "        start_idx = 0\n",
        "        end_idx = len(lines)\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            if \"START OF\" in line.upper() and \"PROJECT GUTENBERG\" in line.upper():\n",
        "                start_idx = i + 1\n",
        "                break\n",
        "\n",
        "        for i in range(len(lines)-1, -1, -1):\n",
        "            if \"END OF\" in lines[i].upper() and \"PROJECT GUTENBERG\" in lines[i].upper():\n",
        "                end_idx = i\n",
        "                break\n",
        "\n",
        "        clean_content = '\\n'.join(lines[start_idx:end_idx])\n",
        "\n",
        "        # Çok agresif temizlik - sadece basit karakterler\n",
        "        import re\n",
        "        clean_content = re.sub(r'[^a-zA-Z0-9\\s\\.\\!\\?\\,]', '', clean_content)\n",
        "        clean_content = re.sub(r'\\s+', ' ', clean_content)\n",
        "        clean_content = clean_content.lower().strip()\n",
        "\n",
        "        # En az 1000 karakter kontrol\n",
        "        if len(clean_content) < 1000:\n",
        "            logger.warning(\"Metin çok kısa, örnek metin kullanılıyor\")\n",
        "            clean_content = (\"alice was beginning to get very tired of sitting by her sister on the bank, \" +\n",
        "                           \"and of having nothing to do. once or twice she had peeped into the book her sister \" +\n",
        "                           \"was reading, but it had no pictures or conversations in it. \") * 50\n",
        "\n",
        "        logger.info(f\"Temizlenmiş metin uzunluğu: {len(clean_content)} karakter\")\n",
        "        return clean_content\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Metin işleme hatası: {e}\")\n",
        "        # Güvenli fallback\n",
        "        fallback_text = (\"the quick brown fox jumps over the lazy dog. \" +\n",
        "                        \"alice was beginning to get very tired. \" +\n",
        "                        \"she found herself falling down a rabbit hole. \") * 200\n",
        "        return fallback_text\n",
        "\n",
        "# Güvenli dataset hazırlama\n",
        "def prepare_dataset(text, config):\n",
        "    \"\"\"NaN'ları önleyecek dataset hazırlama\"\"\"\n",
        "\n",
        "    # Çok basit tokenizer\n",
        "    vectorize_layer = layers.TextVectorization(\n",
        "        standardize='lower_and_strip_punctuation',\n",
        "        max_tokens=config[\"VOCAB_SIZE\"],\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=None,\n",
        "        split='whitespace'\n",
        "    )\n",
        "\n",
        "    # Metni basit şekilde böl\n",
        "    words = text.split()\n",
        "\n",
        "    # Çok kısa metin kontrolü\n",
        "    if len(words) < 1000:\n",
        "        logger.warning(\"Metin çok kısa, uzatılıyor\")\n",
        "        words = words * (1000 // len(words) + 1)\n",
        "\n",
        "    # Basit cümleler oluştur\n",
        "    sentences = []\n",
        "    for i in range(0, len(words) - 10, 5):  # Her 5 kelimede bir\n",
        "        sentence = ' '.join(words[i:i+15])  # 15 kelimelik cümleler\n",
        "        sentences.append(sentence)\n",
        "\n",
        "    logger.info(f\"Toplam {len(sentences)} cümle oluşturuldu\")\n",
        "\n",
        "    # Tokenizer'ı eğit\n",
        "    vectorize_layer.adapt(sentences[:1000])  # İlk 1000 cümle\n",
        "\n",
        "    vocab = vectorize_layer.get_vocabulary()\n",
        "    index_to_word = {i: w for i, w in enumerate(vocab)}\n",
        "\n",
        "    logger.info(f\"Vocabulary boyutu: {len(vocab)}\")\n",
        "\n",
        "    # Tüm metni tokenize et\n",
        "    all_ids = vectorize_layer([text])[0].numpy()\n",
        "\n",
        "    # NaN kontrol\n",
        "    if np.any(np.isnan(all_ids)) or np.any(np.isinf(all_ids)):\n",
        "        logger.error(\"Tokenization'da NaN/Inf bulundu!\")\n",
        "        return None, None, None, None, 0, 0\n",
        "\n",
        "    # Çok basit sequence oluşturma\n",
        "    sequences = []\n",
        "    for i in range(0, len(all_ids) - config[\"MAX_LEN\"] - 1, config[\"MAX_LEN\"] // 2):  # Overlap ile\n",
        "        seq = all_ids[i:i + config[\"MAX_LEN\"] + 1]\n",
        "        if len(seq) == config[\"MAX_LEN\"] + 1:  # Tam uzunluk kontrolü\n",
        "            sequences.append(seq)\n",
        "\n",
        "    logger.info(f\"Toplam {len(sequences)} sequence oluşturuldu\")\n",
        "\n",
        "    if len(sequences) < 100:\n",
        "        logger.error(\"Çok az sequence oluşturuldu!\")\n",
        "        return None, None, None, None, 0, 0\n",
        "\n",
        "    # Dataset oluştur\n",
        "    sequences = np.array(sequences)\n",
        "\n",
        "    # NaN kontrol\n",
        "    if np.any(np.isnan(sequences)) or np.any(np.isinf(sequences)):\n",
        "        logger.error(\"Sequences'da NaN/Inf bulundu!\")\n",
        "        return None, None, None, None, 0, 0\n",
        "\n",
        "    def split_input_target(sequence):\n",
        "        input_seq = sequence[:-1]\n",
        "        target_seq = sequence[1:]\n",
        "        return input_seq, target_seq\n",
        "\n",
        "    inputs = sequences[:, :-1]\n",
        "    targets = sequences[:, 1:]\n",
        "\n",
        "    # Train/validation split\n",
        "    train_size = int(0.9 * len(sequences))\n",
        "\n",
        "    train_inputs = inputs[:train_size]\n",
        "    train_targets = targets[:train_size]\n",
        "    val_inputs = inputs[train_size:]\n",
        "    val_targets = targets[train_size:]\n",
        "\n",
        "    # Dataset oluştur\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_targets))\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((val_inputs, val_targets))\n",
        "\n",
        "    # Batching\n",
        "    train_dataset = (\n",
        "        train_dataset\n",
        "        .shuffle(1000)\n",
        "        .batch(config[\"BATCH_SIZE\"], drop_remainder=True)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    val_dataset = (\n",
        "        val_dataset\n",
        "        .batch(config[\"BATCH_SIZE\"], drop_remainder=True)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    return train_dataset, val_dataset, vectorize_layer, index_to_word, len(train_inputs), len(val_inputs)\n",
        "\n",
        "# Çok güvenli Transformer Block\n",
        "class SafeTransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.rate = rate\n",
        "\n",
        "        # Güvenli attention\n",
        "        self.att = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim // num_heads,\n",
        "            dropout=rate\n",
        "        )\n",
        "\n",
        "        # Güvenli FFN - küçük ağırlıklar\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\",\n",
        "                        kernel_initializer=\"truncated_normal\",\n",
        "                        bias_initializer=\"zeros\"),\n",
        "            layers.Dropout(rate),\n",
        "            layers.Dense(embed_dim,\n",
        "                        kernel_initializer=\"truncated_normal\",\n",
        "                        bias_initializer=\"zeros\")\n",
        "        ])\n",
        "\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Güvenli attention\n",
        "        attn_output = self.att(\n",
        "            inputs, inputs,\n",
        "            use_causal_mask=True,\n",
        "            training=training\n",
        "        )\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        # Güvenli FFN\n",
        "        ffn_output = self.ffn(out1, training=training)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Güvenli Embedding\n",
        "class SafeTokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.maxlen = maxlen\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Güvenli embedding - küçük değerler\n",
        "        self.token_emb = layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embed_dim,\n",
        "            embeddings_initializer=\"truncated_normal\"\n",
        "        )\n",
        "        self.pos_emb = layers.Embedding(\n",
        "            input_dim=maxlen,\n",
        "            output_dim=embed_dim,\n",
        "            embeddings_initializer=\"truncated_normal\"\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        positions = tf.range(start=0, limit=seq_len, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "# Güvenli model\n",
        "def create_safe_model(config):\n",
        "    \"\"\"NaN'ları önleyecek güvenli transformer\"\"\"\n",
        "    inputs = layers.Input(shape=(config[\"MAX_LEN\"],), dtype=tf.int32)\n",
        "\n",
        "    # Güvenli embedding\n",
        "    x = SafeTokenAndPositionEmbedding(\n",
        "        config[\"MAX_LEN\"],\n",
        "        config[\"VOCAB_SIZE\"],\n",
        "        config[\"EMBED_DIM\"]\n",
        "    )(inputs)\n",
        "\n",
        "    x = layers.Dropout(config[\"DROPOUT_RATE\"])(x)\n",
        "\n",
        "    # Transformer blocks\n",
        "    for i in range(config[\"NUM_TRANSFORMER_BLOCKS\"]):\n",
        "        x = SafeTransformerBlock(\n",
        "            config[\"EMBED_DIM\"],\n",
        "            config[\"NUM_HEADS\"],\n",
        "            config[\"FF_DIM\"],\n",
        "            config[\"DROPOUT_RATE\"]\n",
        "        )(x)\n",
        "\n",
        "    # Güvenli output\n",
        "    outputs = layers.Dense(\n",
        "        config[\"VOCAB_SIZE\"],\n",
        "        activation=\"softmax\",\n",
        "        kernel_initializer=\"truncated_normal\",\n",
        "        bias_initializer=\"zeros\"\n",
        "    )(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Güvenli metin üretici\n",
        "class SafeTextGenerator:\n",
        "    def __init__(self, model, tokenizer, index_to_word, max_len):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.index_to_word = index_to_word\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def generate(self, start_prompt, num_words=30, temperature=0.8):\n",
        "        \"\"\"Güvenli metin üretimi\"\"\"\n",
        "        try:\n",
        "            # Tokenize\n",
        "            prompt_tokens = self.tokenizer([start_prompt.lower()])\n",
        "            prompt_tokens = tf.squeeze(prompt_tokens, axis=0).numpy()\n",
        "            prompt_tokens = [int(token) for token in prompt_tokens if token != 0]\n",
        "\n",
        "            if len(prompt_tokens) == 0:\n",
        "                prompt_tokens = [1]  # UNK token\n",
        "\n",
        "            generated_words = []\n",
        "\n",
        "            for _ in range(num_words):\n",
        "                # Son max_len token'ı al\n",
        "                current_sequence = prompt_tokens[-(self.max_len):]\n",
        "\n",
        "                # Padding\n",
        "                padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                    [current_sequence],\n",
        "                    maxlen=self.max_len,\n",
        "                    padding='pre'\n",
        "                )\n",
        "\n",
        "                # Tahmin\n",
        "                predictions = self.model.predict(padded_sequence, verbose=0)\n",
        "\n",
        "                # NaN kontrolü\n",
        "                if np.any(np.isnan(predictions)) or np.any(np.isinf(predictions)):\n",
        "                    logger.warning(\"Prediction'da NaN/Inf bulundu, durduruluyor\")\n",
        "                    break\n",
        "\n",
        "                next_token_logits = predictions[0][-1]\n",
        "\n",
        "                # Temperature scaling\n",
        "                next_token_logits = next_token_logits / max(temperature, 0.1)\n",
        "\n",
        "                # Güvenli sampling\n",
        "                try:\n",
        "                    predicted_id = tf.random.categorical(\n",
        "                        tf.expand_dims(next_token_logits, 0), 1\n",
        "                    )[0, 0].numpy()\n",
        "                except:\n",
        "                    # Fallback - en yüksek olasılığı seç\n",
        "                    predicted_id = np.argmax(next_token_logits)\n",
        "\n",
        "                if predicted_id == 0 or predicted_id >= len(self.index_to_word):\n",
        "                    break\n",
        "\n",
        "                prompt_tokens.append(predicted_id)\n",
        "                predicted_word = self.index_to_word.get(predicted_id, \"[UNK]\")\n",
        "\n",
        "                if predicted_word not in [\"[UNK]\", \"\", \" \"]:\n",
        "                    generated_words.append(predicted_word)\n",
        "\n",
        "            return start_prompt + \" \" + \" \".join(generated_words)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Metin üretimi hatası: {e}\")\n",
        "            return start_prompt + \" [GENERATION_ERROR]\"\n",
        "\n",
        "# Ana fonksiyon\n",
        "def main():\n",
        "    try:\n",
        "        logger.info(\"Güvenli metin işleme başlıyor...\")\n",
        "        full_text = download_and_process_texts()\n",
        "\n",
        "        logger.info(\"Güvenli dataset hazırlanıyor...\")\n",
        "        result = prepare_dataset(full_text, config)\n",
        "\n",
        "        if result[0] is None:\n",
        "            logger.error(\"Dataset hazırlanamadı!\")\n",
        "            return\n",
        "\n",
        "        train_dataset, val_dataset, vectorize_layer, index_to_word, train_size, val_size = result\n",
        "\n",
        "        logger.info(\"Güvenli model oluşturuluyor...\")\n",
        "        model = create_safe_model(config)\n",
        "        model.summary()\n",
        "\n",
        "        # ÇOK güvenli optimizer\n",
        "        optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=config[\"LEARNING_RATE\"],\n",
        "            clipnorm=0.5,  # Çok sıkı gradient clipping\n",
        "            clipvalue=0.5   # Ek güvenlik\n",
        "        )\n",
        "\n",
        "        # Compile\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "            metrics=[\"accuracy\"]\n",
        "        )\n",
        "\n",
        "        # NaN detection callback\n",
        "        class NaNTerminateCallback(tf.keras.callbacks.Callback):\n",
        "            def on_batch_end(self, batch, logs=None):\n",
        "                logs = logs or {}\n",
        "                loss = logs.get('loss')\n",
        "                if loss is not None and (np.isnan(loss) or np.isinf(loss)):\n",
        "                    logger.error(f\"NaN/Inf loss bulundu batch {batch}'de: {loss}\")\n",
        "                    self.model.stop_training = True\n",
        "\n",
        "        # Güvenli callbacks\n",
        "        callbacks = [\n",
        "            NaNTerminateCallback(),\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor=\"val_loss\",\n",
        "                patience=5,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.8,\n",
        "                patience=2,\n",
        "                min_lr=0.00001,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Güvenli eğitim\n",
        "        steps_per_epoch = max(1, train_size // config[\"BATCH_SIZE\"])\n",
        "        validation_steps = max(1, val_size // config[\"BATCH_SIZE\"])\n",
        "\n",
        "        logger.info(f\"Güvenli eğitim başlıyor - {steps_per_epoch} steps/epoch\")\n",
        "\n",
        "        history = model.fit(\n",
        "            train_dataset,\n",
        "            validation_data=val_dataset,\n",
        "            epochs=config[\"EPOCHS\"],\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            validation_steps=validation_steps,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Güvenli test\n",
        "        logger.info(\"Güvenli test başlıyor...\")\n",
        "        generator = SafeTextGenerator(model, vectorize_layer, index_to_word, config[\"MAX_LEN\"])\n",
        "\n",
        "        test_prompts = [\n",
        "            \"alice was beginning\",\n",
        "            \"the rabbit hole\",\n",
        "            \"she found herself\"\n",
        "        ]\n",
        "\n",
        "        for prompt in test_prompts:\n",
        "            generated = generator.generate(prompt, num_words=20, temperature=0.7)\n",
        "            print(f\"\\n--- PROMPT: {prompt} ---\")\n",
        "            print(generated)\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        logger.info(\"Güvenli eğitim tamamlandı!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Ana hata: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "eoD-iJhz13mw",
        "outputId": "a45b4070-65f9-4f70-f41e-0f4efa679fa8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ safe_token_and_position_embedd… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │       \u001b[38;5;34m322,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mSafeTokenAndPositionEmbedding\u001b[0m) │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ safe_transformer_block          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m49,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mSafeTransformerBlock\u001b[0m)          │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ safe_transformer_block_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m49,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mSafeTransformerBlock\u001b[0m)          │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m5000\u001b[0m)       │       \u001b[38;5;34m325,000\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ safe_token_and_position_embedd… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">322,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SafeTokenAndPositionEmbedding</span>) │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ safe_transformer_block          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SafeTransformerBlock</span>)          │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ safe_transformer_block_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SafeTransformerBlock</span>)          │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">325,000</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m747,016\u001b[0m (2.85 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">747,016</span> (2.85 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m747,016\u001b[0m (2.85 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">747,016</span> (2.85 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Ana hata: Only one of `clipnorm`, `clipvalue` and `global_clipnorm` can be set. Received: clipnorm=0.5, clipvalue=0.5, global_clipnorm=None\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-1-2012139818.py\", line 411, in main\n",
            "    optimizer = tf.keras.optimizers.Adam(\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/adam.py\", line 62, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/optimizer.py\", line 21, in __init__\n",
            "    super().__init__(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py\", line 134, in __init__\n",
            "    raise ValueError(\n",
            "ValueError: Only one of `clipnorm`, `clipvalue` and `global_clipnorm` can be set. Received: clipnorm=0.5, clipvalue=0.5, global_clipnorm=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LWT4pDff3zQr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}